# llm-eval-lab
Custom evaluation framework to analyze failure modes in large language models beyond standard benchmarks.
